# Build a question and answer system to query your books using Langchain and Azure OpenAI

## Introduction
In this article, We will explain how to build a powerful question and answer system that leverages OpenAI, LangChain, and Pinecone vector store to query your books effectively. This integration of cutting-edge technologies empowers users to search, retrieve, and analyze information from books with ease and precision.

## Understanding the Components:

### 1.1 Langchain:
LangChain is a user-friendly open-source framework designed to streamline the creation of applications utilizing large language models (LLMs) like OpenAI. This enables the development of responsive applications that leverage the latest advancements in natural language processing, empowering you to build dynamic, data-driven solutions.

The Langchain library's fundamental concept revolves around the ability to link various components together in order to create more sophisticated applications utilizing LLMs. These chains can include multiple components sourced from different modules:

- Prompt templates: Templates designed for various prompt types such as chatbot-style interactions, question-answering, and more.

- Text Splitters: Often times you want to split large text documents into smaller chunks to better work with language models. TextSplitters are responsible for splitting up a document into smaller documents.

- Document Loaders: LangChainâ€™s Document Loaders and Utils modules simplify data access and computation. Document loaders convert diverse data sources into text for processing, while the utils module offers interactive system sessions and code snippets for mathematical computations. 

- Vectorstores: The widely used index type involves generating numerical embeddings for each document using an Embedding Model. These embeddings, along with the associated documents, are stored in a vectorstore. This vectorstore enables efficient retrieval of relevant documents based on their embeddings. 

- Chains: In LangChain, chains are a series of actions that are linked together to do specific jobs. For example, you might want to get information from a certain website, summarize the text you receive, and then answer questions based on the summary. Chains can also be straightforward, like reading what a user enters, creating a message, and providing a reply.

- Agents: These agents employ LLMs to determine appropriate actions to be taken. Tools such as web search or calculators can be utilized, all integrated within a coherent loop of operations.

- Memory: Both short-term and long-term memory capabilities.

### 1.2 Azure Open AI:
Azure Open AI is a cloud-based service provided by Microsoft that offers a wide range of artificial intelligence (AI) capabilities. The integration of Azure Open AI into our Q&A system allows us to utilize powerful language models, such as GPT-3.5, to generate accurate and contextually relevant responses to user queries.

Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the GPT-3, Codex and Embeddings model series. In addition, the new GPT-4 and ChatGPT (gpt-35-turbo) model series have now reached general availability. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation.

### 1.3 Vector Store (Pinecone):
A vector database indexes and stores vector embeddings for fast retrieval and similarity search, with capabilities like CRUD operations, metadata filtering, and horizontal scaling.

Pinecone is a highly scalable vector database designed for similarity search and recommendation systems. It specializes in handling high-dimensional data, making it an excellent choice for processing and retrieving embeddings generated by language models. By leveraging Pinecone DB, we can efficiently store and search the embeddings of our book data, enhancing the search capabilities of our Q&A system.

## Architecture

### Diagram

!["Diagram demonstrating the architecture."](/Question%20and%20Answering%20System/media/Q%26A%20System.png)

### Flow:

We will follow a four-step process, including extracting book content, splitting the book into smaller chunks, building a semantic index using Azure OpenAI Embeddings and store them in pinecone db, and implementing a question and answer flow

#### Step 1: Extracting the Book Content using LangChain Library
The first step is to extract the content of the book using the LangChain library. LangChain is a powerful natural language processing library that provides functionalities for text extraction, parsing, and language modeling. By utilizing LangChain, we can retrieve the text from the book and prepare it for further processing.

The codebase loads unstructured PDF documents using the `UnstructuredPDFLoader` from the langchain package. The path to the PDF book is provided as a parameter to the loader. The loaded data is then split into smaller text chunks using the `RecursiveCharacterTextSplitter` class.

#### Step 2: Splitting the Book into Smaller Chunks
To enhance contextual understanding for OpenAI, we'll partition the loaded PDF document into smaller "pages," each containing 500 characters. This approach optimizes the performance of OpenAI embeddings, as they excel with shorter text segments. Rather than requiring OpenAI to analyze the entire book for every question, it is more efficient and cost-effective to provide it with a concise and relevant section for processing.

#### Step 3: Building a Semantic Index using Azure OpenAI Embeddings
Now that we have our book content divided into smaller chunks, we can proceed to build a semantic index using Azure OpenAI Embeddings. OpenAI Embeddings are powerful language models that encode text into dense vector representations. These embeddings capture the semantic meaning of the text, allowing us to perform similarity-based searches. By converting each book chunk into its corresponding vector representation, we can store them efficiently in an external vector store like Pinecone.

#### Step 4: Question and Answer Flow
With the semantic index in place, we can now implement the question and answer flow. When a user poses a question, we can leverage the Azure OpenAI language model to encode the question into an embedding vector. This question vector is then compared against the indexed book chunks in the semantic index using similarity search algorithm. The system retrieves the most relevant book chunks based on their semantic similarity to the question. Finally, the retrieved book chunks can be presented to the user as potential answers, or further processing can be applied to extract the most appropriate response.


